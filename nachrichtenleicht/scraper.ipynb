{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NachrichtenLeichtScraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re"
      ],
      "metadata": {
        "id": "p7xmIM-wPmWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General stuff"
      ],
      "metadata": {
        "id": "XyAs8Z7si5ZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FOLDER = \"cache\"\n",
        "PLAIN_TEXT_FOLDER = \"plain_text\""
      ],
      "metadata": {
        "id": "b1BnAUbzgojt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_content(page, params=None):\n",
        "  if not os.path.exists(CACHE_FOLDER):\n",
        "    os.makedirs(CACHE_FOLDER)\n",
        "\n",
        "  #load content\n",
        "  content = \"\"\n",
        "  cache_location = os.path.join(CACHE_FOLDER,page.split('/')[-1].replace('/', '_'))\n",
        "  if params is not None:\n",
        "    params_hash = hash(frozenset(params.items()))\n",
        "    cache_location += \"_\" + str(params_hash)\n",
        "  if not os.path.exists(cache_location):\n",
        "    result = requests.get(page, allow_redirects=True, params=params)\n",
        "    content = result.text\n",
        "    with open(cache_location, 'w') as f:\n",
        "      f.write(content)\n",
        "  else:\n",
        "    with open(cache_location) as f:\n",
        "      content = f.read()\n",
        "  return content"
      ],
      "metadata": {
        "id": "ePKBkeNbhs7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
      ],
      "metadata": {
        "id": "7CIkXkEJx_83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape all regular news"
      ],
      "metadata": {
        "id": "17pijg6ueLWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8HWnkzSu6Kq"
      },
      "outputs": [],
      "source": [
        "ITEMS_PER_LOAD = 100 #Probably 100 is maximum\n",
        "GENRES = [\"nachrichten\",\"kultur-index\",\"sport\",\"vermischtes\"]\n",
        "URL = \"https://www.nachrichtenleicht.de/api/partials/PaginatedArticles_NL?drsearch%3AcurrentItems={}&drsearch%3AitemsPerLoad={}&drsearch%3ApartialProps=%7B%22sophoraId%22%3A%22nachrichtenleicht-{}-100%22%7D&drsearch%3A_ajax=1\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_titles():\n",
        "  all_links = []\n",
        "  all_genres = []\n",
        "  #genre_pages = {}\n",
        "  for genre in GENRES:\n",
        "    current_articles = 0\n",
        "    #load new pages as long as there are any\n",
        "    while True:\n",
        "      current_url = URL.format(current_articles,ITEMS_PER_LOAD,genre)\n",
        "      result = requests.get(current_url, allow_redirects=True)\n",
        "      content = result.content\n",
        "      soup = BeautifulSoup(content)\n",
        "\n",
        "      articles = soup.find_all('article')\n",
        "      if len(articles) == 0:\n",
        "        break\n",
        "        #stop iteration if ther are no more articles\n",
        "      for article in articles:\n",
        "        link = article.find('a', href=True)['href']\n",
        "        all_links.append(link)\n",
        "        all_genres.append(genre)\n",
        "        current_articles += 1\n",
        "\n",
        "      print(current_url)\n",
        "\n",
        "  return all_links, all_genres\n",
        "\n",
        "all_links, all_genres = get_all_titles()"
      ],
      "metadata": {
        "id": "gZQpucNAySpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [slugify(sub.split('/')[-1].replace('.html','')) for sub in all_links]\n",
        "\n",
        "link_dataframe = pd.DataFrame(data={'link':all_links,'title':titles, 'genre': all_genres})\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "quHpqqbTSvOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip previous cells if you allready scraped the links\n",
        "link_dataframe = pd.read_csv('meta.csv')"
      ],
      "metadata": {
        "id": "lrNerdhpWngC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_plain_text(html):\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  for figure in soup.find_all('figure'):\n",
        "    figure.decompose() \n",
        "\n",
        "  header_description = soup.find('p', attrs={'class':\"article-header-description\"}).get_text().strip()\n",
        "  article_details = soup.find('section',attrs={'class':\"b-article-details\"}).get_text().strip()\n",
        "\n",
        "\n",
        "  article = header_description + \"\\n\" + article_details\n",
        "  return article"
      ],
      "metadata": {
        "id": "Qic-hCkyQu1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_plain_text(link, name=None):\n",
        "  content = load_content(link)\n",
        "\n",
        "  if name is None:\n",
        "    name = link.split('/')[-1]\n",
        "\n",
        "  name = name.replace('/','_') + '.txt'\n",
        "\n",
        "  #process article\n",
        "  plain_text = to_plain_text(content)\n",
        "\n",
        "  if not os.path.exists(PLAIN_TEXT_FOLDER):\n",
        "    os.makedirs(PLAIN_TEXT_FOLDER)\n",
        "\n",
        "  #save plain text\n",
        "  with open(os.path.join(PLAIN_TEXT_FOLDER , name), 'w+') as f:\n",
        "    f.write(plain_text)\n",
        "\n",
        "  return {}"
      ],
      "metadata": {
        "id": "t6h8x0EKZTsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all all links\n",
        "\n",
        "total_length = len(link_dataframe.index)\n",
        "for index, row in link_dataframe.iterrows():\n",
        "  link = row['link']\n",
        "  title = row['title']\n",
        "\n",
        "  if link == \"\":\n",
        "    continue\n",
        "  \n",
        "  info = load_and_save_plain_text(link, name=title)\n",
        "\n",
        "  #add additional information to meta file\n",
        "  for key, value in info.items():\n",
        "    link_dataframe.at[index,key] = value\n",
        "\n",
        "  print(f\"Loaded ({index}/{total_length})\")"
      ],
      "metadata": {
        "id": "xo0HyzUZZa5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape \"nachrichtenleicht WÃ¶rterbuch\" (Dictionary)"
      ],
      "metadata": {
        "id": "xKPc8vVYecb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://www.nachrichtenleicht.de/woerterbuch\""
      ],
      "metadata": {
        "id": "vNebG2IOebvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get links for all pages\n",
        "def get_all_dictionary_pages():\n",
        "  result = requests.get(URL, allow_redirects=True)\n",
        "  content = result.content\n",
        "\n",
        "  soup = BeautifulSoup(content)\n",
        "  all_letters = soup.find('ul', attrs={\"class\":\"b-list b-alphabet-links u-space-bottom-xxxl\"})\n",
        "  letter_items = all_letters.find_all('a', href=True)\n",
        "  all_dictionary_pages = []\n",
        "  for letter in letter_items:\n",
        "    all_dictionary_pages.append(letter['href'])\n",
        "\n",
        "  return all_dictionary_pages\n",
        "\n",
        "all_dictionary_pages = get_all_dictionary_pages()"
      ],
      "metadata": {
        "id": "DTe9iOZPgs3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all pages and process them\n",
        "for page in all_dictionary_pages:\n",
        "  content = load_content(URL + page)\n",
        "\n",
        "  soup = BeautifulSoup(content)\n",
        "  dictionary = soup.find('ul', attrs={\"class\":\"b-list b-list-teaser-word\"})\n",
        "  if dictionary == None:\n",
        "    continue\n",
        "  dictionary_items = dictionary.find_all('div', attrs={\"class\":\"b-teaser-word\"})\n",
        "\n",
        "  for item in dictionary_items:\n",
        "    title = slugify(item.find('h3', attrs={\"class\":\"teaser-word-title\"}).get_text().strip())\n",
        "    description = item.find('p', attrs={\"class\":\"teaser-word-description\"}).get_text().strip()\n",
        "    \n",
        "    with open(os.path.join(PLAIN_TEXT_FOLDER , \"dict-\" + title + \".txt\"), 'w+') as f:\n",
        "      f.write(description)\n",
        "\n",
        "    link_dataframe = link_dataframe.append({'title': title, 'link':'', 'genre':'dictionary' }, ignore_index=True)"
      ],
      "metadata": {
        "id": "vZW2QqUTgy_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save scraped data"
      ],
      "metadata": {
        "id": "qOTrxaIyeqQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/corpus.zip /content/plain_text -j\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "bmZmbN5PX0SE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}