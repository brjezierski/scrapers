# -*- coding: utf-8 -*-
"""data_processing_pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FiBRwOS-WCs23RsNeVdv8ylOStwMyjja
"""

import re
import pandas as pd
import unicodedata
import subprocess
import os
import sys

def get_all_corpus_files():
  path_pattern = re.compile(r".\/(.*)\/monolingual")
  all_corpus_files = []

  for path in os.walk('.'):
    if "corpus.zip" in path[2]:
      corpus_name = path_pattern.search(path[0])
      if corpus_name != None:
        all_corpus_files.append(os.path.join(path[0],"corpus.zip"))

  return all_corpus_files
  
def get_all_aligned_files():
  path_pattern = re.compile(r".\/(.*)\/aligned")
  all_corpus_files = []

  for path in os.walk('.'):
    for item in path[2]:
      if item.startswith("aligned") and item.endswith(".csv"):
        corpus_name = path_pattern.search(path[0])
        if corpus_name != None:
            all_corpus_files.append(os.path.join(path[0],item))

  return all_corpus_files

CLEAN_DATA_FODLER = "__clean_data__"

"""#General Preprocessing"""

#TODO remove urls


FIND_SEPERATED_WORDS = r"[A-Za-zÄÖÜäöüß]+((·|-)[A-Za-zÄÖÜäöüß]+)+"

LINE = re.compile(r"--+-\n")

REMOVE = {
  LINE:'\n',
}

def preprocess_text(text):
  for remove_item, replacement in REMOVE.items():
    text = remove_item.sub(replacement, text)

  #replace similar tokens (https://util.unicode.org/UnicodeJsps/confusables.jsp?a=%22&r=None)
  text = text\
      .replace("''", '"').replace('＂','"').replace("〃",'"').replace("ˮ",'"')\
      .replace(" ᳓ ",'"').replace("″",'"').replace("״", '"').replace('‶','"')\
      .replace("˶",'"').replace("ʺ",'"').replace("“",'"').replace("˝",'"')\
      .replace("‟",'"').replace("„",'"').replace("”",'"').replace("’","'").replace("´","'")\
      .replace("–","-").replace("˗","-").replace("‐","-").replace("‑","-").replace("‒","-")\
      .replace("·","-").replace("∙","-").replace("⋅","-")\
      .replace("…","...")\
      .replace("☐","•").replace("●","•")
      
  #remove bullet point accidentially used as hyphen (without removing actual bullet points)
  REMOVE_DOT = re.compile(r"([a-zäöüß]+)•([a-zäöüß]+)", flags=re.IGNORECASE)
  text = REMOVE_DOT.sub("\g<1>-\g<2>",text)
  
  text = text\
      .replace('. •', ', ').replace('- ', ' ').replace(', •', ', ').replace('• ', ' ')\
      .replace('\n', ' ').replace('\t', ' ')#\
      #adding spaces after punctuation leads to "..." -> ". . ."
      #.replace('.', '. ').replace('?', '? ').replace(' ?', '?')
      
  
  text = ' '.join(text.split())
  return text.strip()

"""#Split into sentences"""

def split_into_sentences(text):
  # Zerlegung des Textes in Sätze nach diesen Regeln:
  # (?<!\.\.)\s -> Keine Trennung bei ...
  # (?<!\w\.\w.)\s 		-> Keine Trennung wenn zwei Zeichen mit einem Punkt in der Mitte und am Ende dem Leerzeichen vorausgehen (z.B. etc.)
  # (?<![0-9]\.)\s 		-> Keine Trennung wenn eine Nummer folgend von einem Punkt dem Leerzeichen vorausgeht (9. etc.)
  # (?<![0-9][0-9]\.)\s 	-> Keine Trennung wenn zwei Nummern folgend von einem Punkt dem Leerzeichen vorausgehen (18. etc.)
  # (?<![A-Z]\.)			-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Punkt dem Leerzeichen vorausgehen (W. etc)
  # (?<![A-Z][a-z]\.)\s 	-> Keine Trennung wenn ein Großbuchstabe gefolgt von einem Kleibuchstaben und einem Punkt dem Leerzeichen vorausgehen (Dr. etc)
  # (?<=\.|\?|\!)\s 		-> Trennen wenn ein Punkt, Fragezeichen oder Ausrufezeichen dem Leerzeichen vorausgehen
  
  sentences = re.split(r"(?<!\w\.\w.)(?<![0-9]\.)(?<![0-9][0-9]\.)(?<![A-Z]\.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s", text)
  sentences = [s for s in sentences if s]
  return sentences

"""#Map separated words to uniform format"""


def get_mapping(corpus, already_mapped={}):
  separated_word = unicodedata.normalize("NFC",FIND_SEPERATED_WORDS)

  candidates = already_mapped.copy()
  
  #get all files in corpus
  process = subprocess.Popen([f'unzip -p {corpus} | less'], shell=True, stdout=subprocess.PIPE)
  corpus_text = process.communicate()[0]
  corpus_text = unicodedata.normalize("NFC", corpus_text.decode("utf-8"))

  for text in corpus_text.split('\n'):
    text = preprocess_text(text)
    for match in re.finditer(separated_word, text):
      item = match.group()
      item_key = item.replace('-','').replace('·','').lower()
      if item_key in candidates:
        if item not in candidates[item_key]:
          candidates[item_key][item] = 1
        else:
          candidates[item_key][item] += 1
      else:
          candidates[item_key] = {item:1}
  
  return candidates

def get_all_mappings(all_corpora = []):
  all_mapping_candidates = {}
  for corpus in all_corpora:
    all_mapping_candidates = get_mapping(corpus,all_mapping_candidates)

  return all_mapping_candidates

def select_candidates(candidates):
  candidates = candidates.copy()
  unclear_tokens = []
  for replace_candidate, value in candidates.items():

    #rules:
    #1. choose most common word
    #2. if there are multiple options -> select option with fewest subwords
    #3. if theres still no clear decision -> PRINT it, dont use the word in our mapping
    
    #4. split subwords
    #4.1 for nouns:     make each subwords first char uppercase, concat with "-" 
    #4.1 for non nouns: make each subwords first char lowercase, concat with "-" 

    result = None

    is_noun = True
    subword_count = {}
    min_subword = 100

    max_occurence = max(value.values())

    for candidate in value.keys():
      if candidate[0].islower():
        is_noun = False
      if value[candidate] == max_occurence:
        count_separators = candidate.count('-') + candidate.count('·')
        subword_count[candidate] = count_separators
        if count_separators < min_subword:
          min_subword = count_separators

    unique_candidates_keys = {}
    unique_candidates_names = []
    for key,n in subword_count.items():
      if n > min_subword:
        continue
      current_candidate = key.replace('·','-')
      if current_candidate.lower() not in unique_candidates_keys.keys():
        unique_candidates_keys[current_candidate.lower()] = value[key]
        unique_candidates_names.append(current_candidate)
      else:
        unique_candidates_keys[current_candidate.lower()] += value[key]

    if len(unique_candidates_names) == 1:
      splits = unique_candidates_names[0].split('-')
      if is_noun:
        result = "-".join([x[0].upper() + x[1:] for x in splits])
      else:
        result = "-".join([x.lower() for x in splits])
      
      candidates[replace_candidate] = result
    else:
      #Print warning and remove from dict
      print(f"WARNING Don't know which to choose {replace_candidate} -> {unique_candidates_keys}")
      unclear_tokens.append(replace_candidate)

  for token in unclear_tokens:
    candidates.pop(token)

  return candidates

def generate_mappings(corpora):
  mapping_candidates = get_all_mappings(corpora)
  mapping = select_candidates(mapping_candidates)
  return mapping

def execute_mapping(text, map):
  separated_word = unicodedata.normalize("NFC",FIND_SEPERATED_WORDS)
  split_words = unicodedata.normalize("NFC",r"((\b|[ÄÜÖöäü])[^\s]+(\b|[äöüß]))")

  for match in re.finditer(separated_word, text):
    item = match.group()
    item_clean = item.replace('-','').replace('·','')
    item_key = item_clean.lower()
    if item_key in map:

      text = text.replace(item,map[item_key])

  return text

"""#Preprocessing"""

def preprocess_all(additional_files=[]):
  assert(sys.getdefaultencoding() == "utf-8")
  if not os.path.exists(CLEAN_DATA_FODLER):
      os.makedirs(CLEAN_DATA_FODLER)

  all_corpus_files = get_all_corpus_files()

  print("Process corpora:")
  print(all_corpus_files)

  map = generate_mappings(all_corpus_files)

  for corpus in all_corpus_files:

    meta_data = None
    if os.path.exists(corpus.replace('corpus.zip','meta.csv')):
      meta_data = pd.read_csv(corpus.replace('corpus.zip','meta.csv'))

    corpus_name = corpus.split('/')[-3]


    process = subprocess.Popen([f'unzip -l {corpus}'], shell=True, stdout=subprocess.PIPE)
    files = process.communicate()[0]
    files = files.decode("latin-1")
    files = files.split("\n")
    files = [path.split('   ')[-1] for path in files][3:-3]
    files = [path.replace(' ', '\ ') for path in files if path]

    data = {'topic':[], 'phrase':[], 'phrase_number':[], 'genre': []}

    total_length = len(files)
    for i,path in enumerate(files):
      #load text iteratively from zip file
      process = subprocess.Popen([f'unzip -p {corpus} {path} | less'], shell=True, stdout=subprocess.PIPE)
      text = process.communicate()[0]
      text = unicodedata.normalize("NFC", text.decode())
      #do other processing stuff
      text = preprocess_text(text)

      #map semantically same words to same representation
      text = execute_mapping(text, map)
      #print(path)

      sentences = split_into_sentences(text)

      genre = "-"
      if meta_data is not None:
        genre = meta_data.loc[meta_data['title'] == path.replace('.txt','')]['genre'].values
        if len(genre) != 1:
          genre="-"
        else:
          genre=genre[0]

      data['phrase'].extend(sentences)
      data['phrase_number'].extend(list(range(0,len(sentences))))
      data['topic'].extend([path.replace('.txt','')] * len(sentences))
      data['genre'].extend([genre] * len(sentences))

      if (i % 200 == 0):
        print(f"Processed {corpus_name} ({i}/{total_length-1})")
    
    print(f"Processed {corpus_name} ({total_length})")

    df = pd.DataFrame(data=data)
    df.to_csv(os.path.join(CLEAN_DATA_FODLER, corpus_name + '.csv'), index=False)
    
  all_aligned_files = get_all_aligned_files()
  print(f"Process additional files ({all_aligned_files})")
  for aligned_file in all_aligned_files:
  
    corpus_name = aligned_file.split("/aligned/")[0].split("/")[-1]
    file_name = aligned_file.split("/")[-1]
  
    df = pd.read_csv(aligned_file)
    
    for index, row in df.iterrows():
        text = row['simple_phrase']
        if not isinstance(text, str):
            continue
        
        text = unicodedata.normalize("NFC", text)
        #do other processing stuff
        text = preprocess_text(text)

        #map semantically same words to same representation
        text = execute_mapping(text, map)
        
        df.at[index, 'simple_phrase'] = text
        
    print(f"Processed {corpus_name}_{file_name}")
        
    df.to_csv(os.path.join(CLEAN_DATA_FODLER, corpus_name + "_" + file_name), index=False)

if __name__ == '__main__':
    files_to_process = sys.argv[1:]
    
    print(files_to_process)
    preprocess_all(files_to_process)
