{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NorddeutschlandNews.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "BM-lUVWWlVy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General stuff"
      ],
      "metadata": {
        "id": "oVIkvcIUzNCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FOLDER = \"cache\"\n",
        "PLAIN_TEXT_FOLDER = \"plain_text\""
      ],
      "metadata": {
        "id": "luMeB6xsq82e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_content(page, params=None):\n",
        "  if not os.path.exists(CACHE_FOLDER):\n",
        "    os.makedirs(CACHE_FOLDER)\n",
        "\n",
        "  #load content\n",
        "  content = \"\"\n",
        "  cache_location = os.path.join(CACHE_FOLDER,page.split('/')[-1].replace('/', '_'))\n",
        "  if params is not None:\n",
        "    params_hash = hash(frozenset(params.items()))\n",
        "    cache_location += \"_\" + str(params_hash)\n",
        "  if not os.path.exists(cache_location):\n",
        "    result = requests.get(page, allow_redirects=True, params=params)\n",
        "    content = result.text\n",
        "    with open(cache_location, 'w') as f:\n",
        "      f.write(content)\n",
        "  else:\n",
        "    with open(cache_location) as f:\n",
        "      content = f.read()\n",
        "  return content"
      ],
      "metadata": {
        "id": "62PbFnBrrG0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
      ],
      "metadata": {
        "id": "4-zLXq484mZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape all regular news"
      ],
      "metadata": {
        "id": "-AlWyXEfzTv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL_PREFIX = \"https://www.ndr.de\"\n",
        "URL = \"https://www.ndr.de/fernsehen/barrierefreie_angebote/leichte_sprache/leichtesprachearchiv110_page-{}.html\""
      ],
      "metadata": {
        "id": "w5BRYfe5zs0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_links():\n",
        "\n",
        "  all_links = []\n",
        "\n",
        "  page = 1\n",
        "  while True:\n",
        "\n",
        "    current_url = URL.format(page)\n",
        "    content = load_content(current_url)\n",
        "    soup = BeautifulSoup(content)\n",
        "\n",
        "    article_list = soup.find('section', attrs={'class':\"w100 singlecolumnlist \"})\n",
        "    all_articles = article_list.find_all('div',attrs={'class':\"teaserpadding\"})\n",
        "\n",
        "    if len(all_articles) == 2:\n",
        "      break\n",
        "    for article in all_articles:\n",
        "      link = article.find('a',href=True)\n",
        "      all_links.append(link['href'])\n",
        "\n",
        "    print(current_url)\n",
        "\n",
        "    page += 1\n",
        "\n",
        "  return all_links\n",
        "\n",
        "all_links = get_all_links()"
      ],
      "metadata": {
        "id": "c1qQvtiOkzk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_links = [URL_PREFIX + sub for sub in all_links]\n",
        "titles = [slugify(sub.split('/')[-1].replace('.html','')) for sub in all_links]\n",
        "\n",
        "link_dataframe = pd.DataFrame(data={'link':full_links,'title':titles})\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "Vaf7Nm8gfv9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip previous cells if you allready scraped the links\n",
        "link_dataframe = pd.read_csv('meta.csv')"
      ],
      "metadata": {
        "id": "EWNxeMurR0Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define specific processing\n",
        "\n",
        "FOOTER_1 = re.compile(r\"Diese Nachricht ist vom [0-9]+\\. [A-Za-z]+ 20[0-9][0-9], .* Uhr\\.\")\n",
        "FOOTER_2 = re.compile(r\"Mehr Nachrichten vom .* finden Sie hier\\.\")\n",
        "INDENTATION = re.compile(r\"(?<=\\n)\\s+\")\n",
        "LINE = re.compile(r\"--+-\\n\")\n",
        "\n",
        "REMOVE = {\n",
        "  FOOTER_1:'',\n",
        "  FOOTER_2:'',\n",
        "  INDENTATION:'',\n",
        "  LINE:'\\n',\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "  for remove_item, replacement in REMOVE.items():\n",
        "    text = remove_item.sub(replacement, text)\n",
        "\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "fqfoZUEDd57e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_plain_text(html):\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  article = soup.find('article',attrs={'class':\"w100\"})\n",
        "  \n",
        "  to_remove = []\n",
        "  to_remove.extend(article.find_all('header'))\n",
        "  to_remove.extend(article.find_all('div',attrs={'class': \"module voll\"}))\n",
        "  to_remove.extend(article.find_all('div',attrs={'class': \"module halb\"}))\n",
        "  to_remove.extend(article.find_all('div',attrs={'class': \"contentbox voll infobox\"}))\n",
        "  to_remove.extend(article.find_all('div',attrs={'class': \"contentbox w100 relatedbroadcast\"}))\n",
        "  to_remove.extend(article.find_all('div',attrs={'class': \"meta\"}))\n",
        "  to_remove.extend(article.find_all('script'))\n",
        "  to_remove.extend(article.find_all('div', attrs={'id':\"printbox\"}))\n",
        "\n",
        "  #remove titles\n",
        "  to_remove.extend(article.find_all('h2'))\n",
        "\n",
        "  for remove_tag in to_remove:\n",
        "    if remove_tag is not None:\n",
        "      remove_tag.decompose()\n",
        "\n",
        "  plain_text = article.text.strip()\n",
        "\n",
        "  plain_text = process_text(plain_text)\n",
        "\n",
        "  return plain_text"
      ],
      "metadata": {
        "id": "V3ILNGCOy5cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_plain_text(link, name=None):\n",
        "  content = load_content(link)\n",
        "\n",
        "  if name is None:\n",
        "    name = link.split('/')[-1]\n",
        "\n",
        "  name = name.replace('/','_') + '.txt'\n",
        "\n",
        "  #process article\n",
        "  plain_text = to_plain_text(content)\n",
        "\n",
        "  if not os.path.exists(PLAIN_TEXT_FOLDER):\n",
        "    os.makedirs(PLAIN_TEXT_FOLDER)\n",
        "\n",
        "  #save plain text\n",
        "  with open(os.path.join(PLAIN_TEXT_FOLDER , name), 'w+') as f:\n",
        "    f.write(plain_text)\n",
        "\n",
        "  return {'genre': 'news'}"
      ],
      "metadata": {
        "id": "00rSGgWbi-Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_length = len(link_dataframe.index)\n",
        "for index, row in link_dataframe.iterrows():\n",
        "  link = row['link']\n",
        "  title = row['title']\n",
        "  \n",
        "  info = load_and_save_plain_text(link, name=title)\n",
        "\n",
        "  #add additional information to meta file\n",
        "  for key, value in info.items():\n",
        "    link_dataframe.at[index,key] = value\n",
        "\n",
        "  print(f\"Loaded ({index}/{total_length})\")"
      ],
      "metadata": {
        "id": "K44ggOhKijoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save scraped data"
      ],
      "metadata": {
        "id": "wDE3ZKc92e50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/corpus.zip /content/plain_text -j\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "WYU6iFEXc-Qr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}