{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lebenshilfe.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "42iJG5_tAh-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General stuff"
      ],
      "metadata": {
        "id": "RYSNV3psBQTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FOLDER = \"cache\"\n",
        "PLAIN_TEXT_FOLDER = \"plain_text\""
      ],
      "metadata": {
        "id": "zkBdR1J-BCgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_content(page, params=None):\n",
        "  if not os.path.exists(CACHE_FOLDER):\n",
        "    os.makedirs(CACHE_FOLDER)\n",
        "\n",
        "  #load content\n",
        "  content = \"\"\n",
        "  cache_location = os.path.join(CACHE_FOLDER,page.split('/')[-1].replace('/', '_'))\n",
        "  if params is not None:\n",
        "    params_hash = hash(frozenset(params.items()))\n",
        "    cache_location += \"_\" + str(params_hash)\n",
        "  if not os.path.exists(cache_location):\n",
        "    result = requests.get(page, allow_redirects=True, params=params)\n",
        "    content = result.text\n",
        "    with open(cache_location, 'w') as f:\n",
        "      f.write(content)\n",
        "  else:\n",
        "    with open(cache_location) as f:\n",
        "      content = f.read()\n",
        "  return content"
      ],
      "metadata": {
        "id": "WHzBBOVYBIFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
      ],
      "metadata": {
        "id": "fREQR77HzMLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrape all news"
      ],
      "metadata": {
        "id": "0n0dFyXJBNtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL_PREFIX = \"https://www.lebenshilfe.de\"\n",
        "URL = \"https://www.lebenshilfe.de/suche?tx_solr%5Bfilter%5D%5B0%5D=languagemarker_stringS%3Aeasylanguage&tx_solr%5Bpage%5D={}&tx_solr%5Bq%5D=%2A\""
      ],
      "metadata": {
        "id": "0daL3itPBJ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_links():\n",
        "  all_links = []\n",
        "\n",
        "  page = 1\n",
        "  while True:\n",
        "\n",
        "    current_url = URL.format(page)\n",
        "    content = load_content(current_url)\n",
        "    soup = BeautifulSoup(content)\n",
        "\n",
        "    article_list = soup.find('div', attrs={'class':\"tx_solr\"})\n",
        "    all_articles = article_list.find_all('section',attrs={'class':\"list__item\"})\n",
        "\n",
        "    if len(all_articles) == 0:\n",
        "      break\n",
        "    for article in all_articles:\n",
        "      link = article.find('a',href=True)\n",
        "      all_links.append(link['href'])\n",
        "\n",
        "\n",
        "    page += 1\n",
        "  \n",
        "  return all_links\n",
        "\n",
        "all_links = get_all_links()"
      ],
      "metadata": {
        "id": "qD5C1O4BBNC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_links = [URL_PREFIX + sub for sub in all_links]\n",
        "\n",
        "link_dataframe = pd.DataFrame(data={'link':full_links})\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "43MRNMfyEQMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip previous cells if you allready scraped the links\n",
        "link_dataframe = pd.read_csv('meta.csv')"
      ],
      "metadata": {
        "id": "mXk_5PmOEWcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define specific processing\n",
        "\n",
        "INDENTATION = re.compile(r\"(?<=\\n)\\s+\")\n",
        "LINE = re.compile(r\"Möchtest Du Dir ein Fremd·wort wünschen\\? .*@einfachstars\\.info.*\")\n",
        "DUPLICATED_NEW_LINES = re.compile(r\"(?<=\\n\\n)\\s*\\n\")\n",
        "\n",
        "REMOVE = {\n",
        "  INDENTATION:'',\n",
        "  LINE:'\\n',\n",
        "  DUPLICATED_NEW_LINES:'',\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "\n",
        "  for remove_item,replacement in REMOVE.items():\n",
        "    text = remove_item.sub(replacement, text)\n",
        "\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "l6L-w_HiFEw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_plain_text(html):\n",
        "  soup = BeautifulSoup(html)\n",
        "  article = soup.find('article',attrs={'class':\"article\"})\n",
        "  to_remove = []\n",
        "  \n",
        "  try:\n",
        "    title = article.find(\"h1\").get_text().strip()\n",
        "    if (title == \"Wörterbuch\"):\n",
        "      title = article.find(\"h2\").get_text().strip()\n",
        "      to_remove.extend(article.find_all(\"div\", {\"class\" : \"row textrow textrow--intro mb-5\"}))\n",
        "  except:\n",
        "    print(\"Not a valid article\")\n",
        "    return None, None\n",
        "  \n",
        "  #remove titles\n",
        "  to_remove.extend(article.find_all('h1'))\n",
        "  to_remove.extend(article.find_all('h2'))\n",
        "  to_remove.extend(article.find_all(\"div\", {\"class\" : re.compile('.*widget.*')}))\n",
        "  to_remove.extend(article.find_all(\"a\", {\"class\" : re.compile('.*link.*')}))\n",
        "  to_remove.extend(article.find_all('figure'))\n",
        "  \n",
        "  for remove_tag in to_remove:\n",
        "    if remove_tag is not None:\n",
        "      remove_tag.decompose()\n",
        "\n",
        "  regex = re.compile('.*text.*')\n",
        "  plain_text = \"\"\n",
        "  for article_part in article.find_all(\"div\", {\"class\" : regex}):\n",
        "    # for the case when both a descandant and ancestor divs contain \"text\"\n",
        "    if len(article_part.find_all(\"div\", {\"class\" : regex})) > 0:\n",
        "      continue\n",
        "    plain_text = plain_text + \" \" + article_part.get_text()\n",
        "\n",
        "  plain_text = process_text(plain_text)\n",
        "\n",
        "  return plain_text, title"
      ],
      "metadata": {
        "id": "JG6SU_tPENGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_plain_text(link, name=None):\n",
        "  content = load_content(link)\n",
        "\n",
        "  if name is None:\n",
        "    name = link.split('/')[-1]\n",
        "\n",
        "  name = name.replace('/','_') + '.txt'\n",
        "\n",
        "  #process article\n",
        "  plain_text, title = to_plain_text(content)\n",
        "\n",
        "  if plain_text == None or title == None:\n",
        "    return {}\n",
        "\n",
        "  title = slugify(title)\n",
        "  name = slugify(title) + \".txt\"\n",
        "\n",
        "  if not os.path.exists(PLAIN_TEXT_FOLDER):\n",
        "    os.makedirs(PLAIN_TEXT_FOLDER)\n",
        "\n",
        "  #save plain text\n",
        "  with open(os.path.join(PLAIN_TEXT_FOLDER , name), 'w+') as f:\n",
        "    f.write(plain_text)\n",
        "\n",
        "  return {'genre': 'news', 'title':title}"
      ],
      "metadata": {
        "id": "aylZ76gIEtsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all all links\n",
        "\n",
        "total_length = len(link_dataframe.index)\n",
        "for index, row in link_dataframe.iterrows():\n",
        "  link = row['link']\n",
        "  \n",
        "  info = load_and_save_plain_text(link)\n",
        "\n",
        "  #add additional information to meta file\n",
        "  for key, value in info.items():\n",
        "    link_dataframe.at[index,key] = value\n",
        "\n",
        "  print(f\"Loaded ({index}/{total_length})\")"
      ],
      "metadata": {
        "id": "LSNwtaCeE2G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save scraped data"
      ],
      "metadata": {
        "id": "uSc9qVx_KPbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/corpus.zip /content/plain_text -j\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "l8povKflKRvp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}