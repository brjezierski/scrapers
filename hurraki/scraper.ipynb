{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hurraki.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import urllib\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "7sCsXF391OWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#General stuff"
      ],
      "metadata": {
        "id": "f4vYz9a9d4HC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FOLDER = \"cache\"\n",
        "PLAIN_TEXT_FOLDER = \"plain_text\""
      ],
      "metadata": {
        "id": "FP1zG8_iZ0h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_content(page, params=None):\n",
        "  if not os.path.exists(CACHE_FOLDER):\n",
        "    os.makedirs(CACHE_FOLDER)\n",
        "\n",
        "  #load content\n",
        "  content = \"\"\n",
        "  cache_location = os.path.join(CACHE_FOLDER,page.split('/')[-1].replace('/', '_'))\n",
        "  if params is not None:\n",
        "    params_hash = hash(frozenset(params.items()))\n",
        "    cache_location += \"_\" + str(params_hash)\n",
        "  if not os.path.exists(cache_location):\n",
        "    result = requests.get(page, allow_redirects=True, params=params)\n",
        "    content = result.text\n",
        "    with open(cache_location, 'w') as f:\n",
        "      f.write(content)\n",
        "  else:\n",
        "    with open(cache_location) as f:\n",
        "      content = f.read()\n",
        "  return content"
      ],
      "metadata": {
        "id": "uZDlOYJjd9BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
      ],
      "metadata": {
        "id": "-lEJmm8668Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrape all hurraki data"
      ],
      "metadata": {
        "id": "nbX2oOZ2Z384"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL_ALL_PAGES = \"https://hurraki.de/w/api.php?action=query&format=json&list=allpages&aplimit=500&apfilterredir=nonredirects&apfrom={}\"\n",
        "URL_GET_PAGE = \"https://hurraki.de/w/api.php?action=parse&prop=categories|text&format=json&page={}\""
      ],
      "metadata": {
        "id": "HcN0zkGbZ3QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_titles():\n",
        "\n",
        "  apfrom = \"A\"\n",
        "  all_pages = []\n",
        "\n",
        "  #iterate over all pages (max. 500 entries per page)\n",
        "  while apfrom is not None:\n",
        "\n",
        "    #load data\n",
        "    data = json.loads(load_content(URL_ALL_PAGES.format(apfrom)))\n",
        "    pages = data[\"query\"][\"allpages\"]\n",
        "\n",
        "    for page in pages:\n",
        "      if page[\"title\"].startswith(\"Hurraki:\"):\n",
        "        continue\n",
        "      all_pages.append(page[\"title\"])\n",
        "\n",
        "    if \"continue\" in data.keys():\n",
        "      apfrom = data['continue']['apcontinue']\n",
        "    else:\n",
        "      apfrom = None\n",
        "\n",
        "  return all_pages\n",
        "\n",
        "all_titles = get_all_titles()"
      ],
      "metadata": {
        "id": "QlciAT-K2q9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_links = [URL_GET_PAGE.format(urllib.parse.quote(sub)) for sub in all_titles]\n",
        "\n",
        "slugified_titles = [slugify(title) for title in all_titles]\n",
        "link_dataframe = pd.DataFrame(data={'title':slugified_titles,'link':full_links})\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "ae7BLxYiOFKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip previous cells if you allready scraped the links\n",
        "link_dataframe = pd.read_csv('meta.csv', keep_default_na=False)"
      ],
      "metadata": {
        "id": "sWMJXUsxRxug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define specific processing\n",
        "\n",
        "IMAGE_CAPTION = re.compile(r\"Auf dem Bild ist __*_ zu sehen\")\n",
        "DUPLICATED_NEW_LINES = re.compile(r\"(?<=\\n\\n)\\s*\\n\")\n",
        "UNIFORM_BULLET_POINTS = re.compile(r\"(?<=\\n)\\s*- *\")\n",
        "\n",
        "REMOVE = {\n",
        "  IMAGE_CAPTION:'',\n",
        "  DUPLICATED_NEW_LINES:'',\n",
        "  UNIFORM_BULLET_POINTS:'• ',\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "  for remove_item, replacement in REMOVE.items():\n",
        "    text = remove_item.sub(replacement, text)\n",
        "\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "CvH1S3t0K-R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_plain_text(html):\n",
        "  LONG_DESCRIPTION_SEPARATOR = \"\"\"<span class=\"mw-headline\" id=\"Genaue_Erkl.C3.A4rung\">Genaue Erklärung</span>\"\"\"\n",
        "\n",
        "  #split sections\n",
        "  splitted = html.split(\"<span class=\\\"mw-headline\\\"\")\n",
        "  #reconstruct sections\n",
        "  for i in range(1,len(splitted)):\n",
        "    splitted[i] = \"<span class=\\\"mw-headline\\\"\" + splitted[i]\n",
        "\n",
        "  long_description_html = \"\"\n",
        "  short_description_html = splitted[0]\n",
        "\n",
        "  for section in splitted:\n",
        "    if LONG_DESCRIPTION_SEPARATOR in section:\n",
        "      long_description_html = section\n",
        "      break\n",
        "\n",
        "  html = short_description_html + \"\\n\" + long_description_html\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  #format lists to recognize them with regex\n",
        "  for ul_list in soup.find_all('ul') + soup.find_all('ol'):\n",
        "\n",
        "    for item in ul_list.find_all('li'):\n",
        "      if item.text.strip() != \"\":\n",
        "        item.string = \"• \" + item.text.strip()\n",
        "\n",
        "  #remove all divs\n",
        "  to_remove = []\n",
        "  to_remove.extend(soup.find_all('div'))\n",
        "  to_remove.extend(soup.find_all('table'))\n",
        "  to_remove.extend(soup.find_all('span', attrs={\"class\":\"mw-editsection\"}))\n",
        "  to_remove.extend(soup.find_all('span', attrs={\"class\":\"mw-headline\"}))\n",
        "  to_remove.extend(soup.find_all('h2'))\n",
        "  to_remove.extend(soup.find_all('h3'))\n",
        "  #remove hadlines\n",
        "  for remove_tag in to_remove:\n",
        "    if remove_tag is not None:\n",
        "      remove_tag.decompose() \n",
        "\n",
        "  plain_text = soup.get_text().strip()\n",
        "\n",
        "  plain_text = process_text(plain_text)\n",
        "\n",
        "  return plain_text"
      ],
      "metadata": {
        "id": "FpJVtSP-VYag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_plain_text(link, name=None):\n",
        "\n",
        "  result = json.loads(load_content(link))\n",
        "  if 'parse' not in result:\n",
        "    print(link)\n",
        "  content = result['parse']['text']['*']\n",
        "  categories = []\n",
        "  for item in result['parse']['categories']:\n",
        "    categories.append(item['*'])\n",
        "  category = \"|\".join(categories)\n",
        "\n",
        "  if name is None:\n",
        "    name = link.split('/')[-1]\n",
        "\n",
        "  name = name.replace('/','_') + '.txt'\n",
        "\n",
        "  #process article\n",
        "  plain_text = to_plain_text(content)\n",
        "\n",
        "  if not os.path.exists(PLAIN_TEXT_FOLDER):\n",
        "    os.makedirs(PLAIN_TEXT_FOLDER)\n",
        "\n",
        "  #save plain text\n",
        "  with open(os.path.join(PLAIN_TEXT_FOLDER , name), 'w+') as f:\n",
        "    f.write(plain_text)\n",
        "\n",
        "  return {'genre': category}"
      ],
      "metadata": {
        "id": "jtUb1BXoLhzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load and parse all pages\n",
        "\n",
        "exclude_pages = ['Übung: Demoversion',\n",
        "                 'Übung: jaki',\n",
        "                 'Übung: jan poka',\n",
        "                 'Übung: kala 🐟',\n",
        "                 'Übung: kili 🍎',\n",
        "                 'Übung: kulupu tomo','Übung: ma Apika',\n",
        "                 'Übung: ma Elopa',\n",
        "                 'Übung: ma Italija',\n",
        "                 'Übung: pali',\n",
        "                 'Übung: toki mama',\n",
        "                 'Ubung: kon tawa 🌬️',\n",
        "                 'Grundrechenart',\n",
        "                 'Mom',\n",
        "                 'Jaki',\n",
        "                 'Konsonant',\n",
        "                 'Alice Salomon',\n",
        "                 'Unterstützte Kommunikation']\n",
        "\n",
        "exclude_pages = [slugify(item) for item in exclude_pages]\n",
        "\n",
        "total_length = len(link_dataframe.index)\n",
        "for index, row in link_dataframe.iterrows():\n",
        "  link = row['link']\n",
        "  title = row['title']\n",
        "\n",
        "  if title in exclude_pages:\n",
        "    continue\n",
        "  \n",
        "  info = load_and_save_plain_text(link, name=title)\n",
        "\n",
        "  #add additional information to meta file\n",
        "  for key, value in info.items():\n",
        "    link_dataframe.at[index,key] = value\n",
        "\n",
        "  print(f\"Loaded ({index}/{total_length})\")"
      ],
      "metadata": {
        "id": "KgP4Bj2uf4_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save scraped data"
      ],
      "metadata": {
        "id": "WRuStDBPOyr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/corpus.zip /content/plain_text -j\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "IQfoZhqpOwyn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}