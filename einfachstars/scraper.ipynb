{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Einfachstars.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9BvjB66RWVr"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General stuff"
      ],
      "metadata": {
        "id": "FJoASYphCGKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_FOLDER = \"cache\"\n",
        "PLAIN_TEXT_FOLDER = \"plain_text\""
      ],
      "metadata": {
        "id": "CLhcNTmDVrNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_content(page, params=None):\n",
        "  if not os.path.exists(CACHE_FOLDER):\n",
        "    os.makedirs(CACHE_FOLDER)\n",
        "\n",
        "  #load content\n",
        "  content = \"\"\n",
        "  cache_location = os.path.join(CACHE_FOLDER,page.split('/')[-1].replace('/', '_'))\n",
        "  if params is not None:\n",
        "    params_hash = hash(frozenset(params.items()))\n",
        "    cache_location += \"_\" + str(params_hash)\n",
        "  if not os.path.exists(cache_location):\n",
        "    result = requests.get(page, allow_redirects=True, params=params)\n",
        "    content = result.text\n",
        "    with open(cache_location, 'w') as f:\n",
        "      f.write(content)\n",
        "  else:\n",
        "    with open(cache_location) as f:\n",
        "      content = f.read()\n",
        "  return content"
      ],
      "metadata": {
        "id": "DAq08q3yVtHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def slugify(value, allow_unicode=False):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n",
        "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
        "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
        "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
        "    trailing whitespace, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    value = str(value)\n",
        "    if allow_unicode:\n",
        "        value = unicodedata.normalize('NFKC', value)\n",
        "    else:\n",
        "        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n",
        "    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n",
        "    return re.sub(r'[-\\s]+', '-', value).strip('-_')"
      ],
      "metadata": {
        "id": "jQYfSwoq0Osh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape all regular news"
      ],
      "metadata": {
        "id": "UVbmPalwcpAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL_PREFIX = \"https://einfachstars.info\"\n",
        "URL = \"https://einfachstars.info/blog/index.html?page={}\""
      ],
      "metadata": {
        "id": "EMGHuFRSVvIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_links():\n",
        "\n",
        "  all_links = []\n",
        "\n",
        "  page = 1\n",
        "  while True:\n",
        "\n",
        "    current_url = URL.format(page)\n",
        "    content = load_content(current_url)\n",
        "    soup = BeautifulSoup(content)\n",
        "\n",
        "    all_articles = soup.find_all('article',attrs={'class':\"post\"})\n",
        "\n",
        "    if len(all_articles) == 0:\n",
        "      break\n",
        "    for article in all_articles:\n",
        "      link = article.find('h1',attrs={'class':\"title\"}).find('a',href=True)\n",
        "      all_links.append(link['href'])\n",
        "\n",
        "    print(current_url)\n",
        "\n",
        "    page += 1\n",
        "\n",
        "  return all_links\n",
        "\n",
        "all_links = get_all_links()"
      ],
      "metadata": {
        "id": "JYwiQGubV6p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_links = [URL_PREFIX + sub for sub in all_links]\n",
        "titles = [slugify(sub.split('/')[-1].replace('.html','')) for sub in all_links]\n",
        "\n",
        "link_dataframe = pd.DataFrame(data={'link':full_links,'title':titles})\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "bfCm6j4Rcln-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#skip previous cells if you allready scraped the links\n",
        "link_dataframe = pd.read_csv('meta.csv')"
      ],
      "metadata": {
        "id": "uAhTFvz4RrYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define specific processing\n",
        "\n",
        "INTRODUCTION = re.compile(r\"In der Leichten Sprache werden nur wenige Fremd·wörter benutzt\\.(\\n|.)*Heute:.*\\?\")\n",
        "FOOTER = re.compile(r\"Möchtest Du Dir ein Fremd·wort wünschen\\? .*@einfachstars\\.info.*\")\n",
        "HINT = re.compile(r\"Diese Erklärung hat sich eine Leserin von Einfachstars gewünscht.\")\n",
        "\n",
        "IMAGE_CAPTION_1 = re.compile(r\"Hier ist ein Bild .*:\\s*\\n\")\n",
        "IMAGE_CAPTION_2 = re.compile(r\"So sieht .* aus:\\s*\\n\")\n",
        "IMAGE_CAPTION_3 = re.compile(r\"(Hier|In).*kann man .*sehen:\")\n",
        "\n",
        "BRACKETS = re.compile(r\"\\[[^\\[\\]]*\\]\")\n",
        "DUPLICATED_NEW_LINES = re.compile(r\"(?<=\\n\\n)\\s*\\n\")\n",
        "\n",
        "REMOVE = {\n",
        "  INTRODUCTION:'',\n",
        "  FOOTER:'',\n",
        "  HINT:'',\n",
        "  IMAGE_CAPTION_1:'',\n",
        "  IMAGE_CAPTION_2:'',\n",
        "  IMAGE_CAPTION_3:'',\n",
        "  BRACKETS:'',\n",
        "  DUPLICATED_NEW_LINES:'',\n",
        "}\n",
        "\n",
        "def process_text(text):\n",
        "\n",
        "  for remove_item,replacement in REMOVE.items():\n",
        "    text = remove_item.sub(replacement, text)\n",
        "\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "y1EA-FoAR2sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_plain_text(html):\n",
        "  html = html.replace('<strong>','')\n",
        "  html = html.replace('</strong>','')\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "  article = soup.find('div',attrs={'class':\"body clear\"})\n",
        "\n",
        "  to_remove = []\n",
        "  to_remove.extend(article.find_all('div'))\n",
        "  to_remove.extend(article.find_all('iframe'))\n",
        "  to_remove.extend(article.find_all('blockquote'))\n",
        "  to_remove.extend(article.find_all('p', attrs={'class':\"mailtext\"}))\n",
        "\n",
        "  for remove_tag in to_remove:\n",
        "    if remove_tag is not None:\n",
        "      remove_tag.decompose()\n",
        "\n",
        "  for br in article.find_all(\"br\"):\n",
        "    br.replace_with(\"\\n\")\n",
        "\n",
        "  plain_text = []\n",
        "  for line in article.find_all(['p', 'ul']):\n",
        "    if line.name == 'ul':\n",
        "      for item in line.find_all('li'):\n",
        "        plain_text.append(\"• \" + item.get_text(separator=\" \").strip())\n",
        "    else:\n",
        "      for sub_line in line.get_text(separator=\" \").split('\\n'):\n",
        "        plain_text.append(sub_line.strip())\n",
        "  \n",
        "  plain_text = \"\\n\".join(plain_text)\n",
        "\n",
        "  plain_text = process_text(plain_text)\n",
        "\n",
        "  return plain_text"
      ],
      "metadata": {
        "id": "pDnNwdawcx4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_save_plain_text(link, name=None):\n",
        "  content = load_content(link)\n",
        "\n",
        "  if name is None:\n",
        "    name = link.split('/')[-1]\n",
        "\n",
        "  name = name.replace('/','_') + '.txt'\n",
        "\n",
        "  #process article\n",
        "  plain_text = to_plain_text(content)\n",
        "\n",
        "  if not os.path.exists(PLAIN_TEXT_FOLDER):\n",
        "    os.makedirs(PLAIN_TEXT_FOLDER)\n",
        "\n",
        "  #save plain text\n",
        "  with open(os.path.join(PLAIN_TEXT_FOLDER , name), 'w+') as f:\n",
        "    f.write(plain_text)\n",
        "\n",
        "  return {'genre': 'news'}"
      ],
      "metadata": {
        "id": "qy6GUipBOB-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load all all links\n",
        "\n",
        "total_length = len(link_dataframe.index)\n",
        "for index, row in link_dataframe.iterrows():\n",
        "  link = row['link']\n",
        "  title = row['title']\n",
        "  \n",
        "  info = load_and_save_plain_text(link, name=title)\n",
        "\n",
        "  #add additional information to meta file\n",
        "  for key, value in info.items():\n",
        "    link_dataframe.at[index,key] = value\n",
        "\n",
        "  print(f\"Loaded ({index}/{total_length})\")"
      ],
      "metadata": {
        "id": "waju6CIYd9jQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save scraped data"
      ],
      "metadata": {
        "id": "UHTbdDZU3Qfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/corpus.zip /content/plain_text -j\n",
        "link_dataframe.to_csv('meta.csv', index=False)"
      ],
      "metadata": {
        "id": "rrQiPDb-UtB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}